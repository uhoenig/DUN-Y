---
title: "American style Options - Regression methods on Pricing"
author: "Team DUN-Y - Dom, Uwe, Niti, YQ"
date: "March 2016"
output: rmarkdown::pdf_document
vignette: >
  %\VignetteIndexEntry{DUN Vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


## Abstract (CHANGE)
In this report we present a detailed description of the methods used for the in-class Kaggle Online News Popularity competition. Our final model is based on a popular multi-layer approach, inspired by the winners Gilberto Titericz & Stanislav Semenov of the Otto Group Product Classification challenge on Kaggle. The solution is based on a two-fold approach: Feature engineering and machine learning techniques. The latter consists of a 3-layer learning architecture as shown in the picture below. The result on the private Kaggle scoreboard was 52.83% accuracy.

## Introduction (CHANGE)
An American option is like a European option except that the holder may exercise at any
time between the start date and the expiry date. An American call or put option is a
contract such that the owner may (without obligation) buy or sell some prescribed
asset (called the underlying) S from the writer at any time (expiry date) T between the
start date and a prescribed expiry date in the future, not just at expiry, and at a
prescribed price (exercise or strike price) K. The exercise time τ can be represented as
a stopping time; so that American options are an example of optimal stopping time
problems.
Without prescribed exercise time makes it much harder to evaluate these options. The
holder of an American option is thus faced with the dilemma of deciding when, if at
all, to exercise. If, at time t, the option is out-of-the-money then it is clearly best not to
exercise. However, if the option is in-the-money it may be beneficial to wait until a
later time where the payoff might be even bigger. 

## Data and considered models
### Created features
After testing various interactive variables we found these to be important:

- LDA_02 / LDA_03
- LDA_03 / LDA_02
- LDA_03 / LDA_04

Plotting these features against article popularity shows some significant differences between different classes. However, the improvement in the first layer was too small and we dropped them.
These features were ranked important by Random Forests but not by Xgboost. 

![alt text](rf_importance.png)


![alt text](xgb_importance.png)

We also explored several binary features (1 if greater than zero): 

- min keyword
- number of self references
- number of images
- number of videos

We also considered logs of kw_avg_avg and kw_max_avg without any improvement.

###Titles and Text Mining
We also extracted titles from the url column. We discovered that n_tokens_title feature (number of words in title) is not always correct. Creating our own title length feature did not contribute to accuracy. We looked at most common words in titles, and after excluding connectors and articles we explored words like “google”, “facebook”,  “video” etc. We were looking particularly at word frequency among class labels and tried to find common words that had significantly different class distribution. However, most common words appear in less than 1000 titles (<0.03) so they did not end up contributing to accuracy.

###Feature selection
We tried different models on subset of features without improvement.

- top n features given by Random Forest
- top n features given by XGBoost
- top n features given by Gibbs Sampler (mombf package)

### Dimensionality reduction
- PCA: top n eigenvalues

- t-Distributed Stochastic Neighbor Embedding (tsne package). The plot below shows that the data will not be easily classified by a k-nn algorithm. 

![alt text](tsne.png)

### Considered models
K-Nearest Neighbors metafeatures (K $\in$ ${1,2,4,8,16,32,64,128,256}$). Cross validated accuracy is below 49%. Poor performance in correlation with poor performance of tSNE.

GBM (gbm package) - already did adaBoost and xgboost. Accuracy bellow 50.5%.


##Method

![alt text](architecture.png)

In the first layer we used five folds to create metafeatures using 4 different classifiers. The metafeatures for the test set were created by using all available training data. After a thorough investigation of different types of classifiers for the first level we decided to stick with the following list:

- Random Forest (1000 trees, probability output)
- Xgboost (250 rounds, softprob, optimised using grid search in caret package)
- AdaBoost (250 rounds, maboost package)
- Multinomial logistic regression (glmnet package)

In the second layer we again optimize parameters of Xgboost using only metafeatures created in the first layer. We do the same for h2o Neural Net. Both models are then trained using the optimal parameters and with the softprob output.

In the third layer we use arithmetic/geometric averaging to combine Xgboost and Neural Networks to produce the final classification.
Our final model is based on a quite popular multi-layered approach. In the first layer we used fixed five folds to create Metafeatures using 4 different classifiers.  Metafeatures for the test set were created by training on the full train set. We considered several different classifiers for the first level and in the end kept the following: 
Random Forest (1000 trees, probability output)
Xgboost (250 rounds, softprob, optimised using grid search in caret package)
AdaBoost (250 rounds, maboost package)
Multinomial logistic (glmnet package)


#Conclusion

FD and tree techniques are efficient methods to price American options with single
underlying security whereas simulation works well better for multi-asset American
options. In this study, we focused on the LSM algorithm of Longstaff and Schwartz
(2001), which is a regression-based Monte Carlo simulation method for pricing American
options.
When we know the early exercise boundary, pricing an American option is quite
easy. In the one-dimensional case, the estimate of LSM for the early exercise boundary
at the last time step does not match with the result of Black-Scholes formula and
Newton Raphson method. This shows that LSM cannot estimate the boundary well.
We tried to reduce the inefficiency of LSM algorithm about the input selection for the
regression and improved the algorithm. It now estimates the price of an American
option with less computational cost and more accurately than the LSM algorithm.
Furthermore, we coded an optimization approach, which maximizes the total value at
each time step, and we always had results higher than those of FD method. Therefore,
it might be possible to use it as an upper bound of the American option price


For multi-asset American options, we extended the implementation of the LSM
algorithm. We analyzed how many early exercise points should be considered for the
option to be close to the American price and we showed that the price estimates in
the literature are actually too low for American options on the maximum of two or
five assets as the number of time steps was selected too small. For American spread
options, we tested different sets of basis functions and determined a useful one. We
also applied variance reduction techniques for multi-asset options. We showed that it is
possible to use Kirk’s approximation for European spread option as a control variable
on its American equivalent while applying control variates (CV). Considerable variance
reduction is achieved by using antithetic variates (AV) and CV for maximum options
on two assets, AV for maximum options on five assets and CV for spreads